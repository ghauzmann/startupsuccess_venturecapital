{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import dataframe_image as dfi\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Relevant Datasets as Pandas Dataframes\n",
    "# Crunchbase December 4th 2015 \n",
    "\n",
    "# Companies\n",
    "companies_df = pd.read_csv(\"Data/companies.csv\")\n",
    "\n",
    "# Additions\n",
    "additions_df = pd.read_csv(\"Data/additions.csv\")\n",
    "\n",
    "# Acquisitions\n",
    "acquisitions_df = pd.read_csv(\"Data/acquisitions.csv\")\n",
    "\n",
    "# Investments\n",
    "investments_df = pd.read_csv(\"Data/investments.csv\")\n",
    "\n",
    "# Rounds\n",
    "rounds_df = pd.read_csv(\"Data/rounds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting DF shapes\n",
    "\n",
    "print(\"Companies shape: \", companies_df.shape)\n",
    "print(\"Additions shape: \", additions_df.shape)\n",
    "print(\"Acquisitions shape: \", acquisitions_df.shape)\n",
    "print(\"Investments shape: \", investments_df.shape)\n",
    "print(\"Rounds shape: \", rounds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inspecting columns and general info\n",
    "\n",
    "print(\"\\033[1m Companies Info: \\033[0m\") \n",
    "companies_df.info()\n",
    "\n",
    "print(\"\\n\\033[1m Additions Info: \\033[0m\") \n",
    "additions_df.info()\n",
    "\n",
    "print(\"\\n\\033[1m Acquisitions Info: \\033[0m\") \n",
    "acquisitions_df.info()\n",
    "\n",
    "print(\"\\n\\033[1m Investments Info: \\033[0m\") \n",
    "investments_df.info()\n",
    "\n",
    "print(\"\\n\\033[1m Rounds Info: \\033[0m\") \n",
    "rounds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting sample of 5 entries from companies.csv as png to use in report\n",
    "sample_comp_df = companies_df.sample(n=5)\n",
    "\n",
    "dfi.export(sample_comp_df,\"Sample_companies.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecing number of NaN values in every column in Companies\n",
    "companies_df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all NaN in \"first_funding_at\" and \"name\" columns\n",
    "cleaned_companies_df = companies_df.dropna(subset=[\"first_funding_at\", \"name\"])\n",
    "\n",
    "# Inspecting shape of companies df and NaN columns\n",
    "print(cleaned_companies_df.shape)\n",
    "print(\"\\n\\033[1mNaN values in every column in Companies after cleaning names and first funding at:\\n\\033[0m\", cleaned_companies_df.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting range of values in \"first_funding_at\" in Companies DF\n",
    "cleaned_companies_df.sort_values(\"first_funding_at\", ascending=True)\n",
    "\n",
    "# Inspecting range of values in \"last_funding_at\" in Companies DF\n",
    "cleaned_companies_df.sort_values(\"last_funding_at\", ascending=False)\n",
    "\n",
    "# Removing entries where \"first_funding_at\" and \"last_funding_at\" is not\n",
    "# in the 20th or 21st century as this simply dos not make sense\n",
    "cleaned_companies_df = cleaned_companies_df[cleaned_companies_df.first_funding_at.str.match(r\"(19)|(20)\")]\n",
    "cleaned_companies_df = cleaned_companies_df[cleaned_companies_df.last_funding_at.str.match(r\"(19)|(20)\")]\n",
    "\n",
    "cleaned_companies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting \"first_funding_at\" and \"last_funding_at\" to Pandas Date Time Objects to perform future time operations\n",
    "cleaned_companies_df[\"first_funding_at\"] = pd.to_datetime(cleaned_companies_df[\"first_funding_at\"])\n",
    "cleaned_companies_df[\"last_funding_at\"] = pd.to_datetime(cleaned_companies_df[\"last_funding_at\"])\n",
    "\n",
    "cleaned_companies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping URL Column as it does not provide much\n",
    "cleaned_companies_df.drop(axis=1, columns=[\"homepage_url\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter companies to \"first_funding_at\" to be post 2000\n",
    "# as this can be seen to be the more modern era of VC and Startups after the dot com crash\n",
    "cleaned_companies_df = cleaned_companies_df[cleaned_companies_df['first_funding_at'] >= '2000-01-01']\n",
    "cleaned_companies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling '-' with NaNs to be treated as numeric in \"funding_total_usd\" column\n",
    "cleaned_companies_df.funding_total_usd.replace('-', np.nan, inplace=True)\n",
    "\n",
    "# Converting \"funding_total_usd\" and \"funding_rounds\" columns from\n",
    "# objects to float and integer respeectively to perform operations\n",
    "cleaned_companies_df.funding_total_usd = pd.to_numeric(cleaned_companies_df.loc[:, \"funding_total_usd\"])\n",
    "cleaned_companies_df.funding_rounds = pd.to_numeric(cleaned_companies_df.loc[:, \"funding_rounds\"])\n",
    "\n",
    "cleaned_companies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting \"status\" column value counts \n",
    "# as this indicates whether the startups are success or not \n",
    "cleaned_companies_df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting Pandas to not display max rows\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Inspecting which countries are part of this dataset and their value counts\n",
    "cleaned_companies_df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a subset dataset of only US and European countries\n",
    "# EEA plus Israel\n",
    "eu_us_countries = [\n",
    "    \"GBR\", \"FRA\", \"DEU\", \"ISR\", \"ESP\", \"NLD\",\n",
    "    \"SWE\", \"IRL\", \"ITA\", \"CHE\", \"DNK\", \"FIN\",\n",
    "    \"BEL\", \"POL\", \"NOR\", \"AUT\", \"PRT\", \"BGR\",\n",
    "    \"EST\", \"CZE\", \"HUN\", \"LVA\", \"LTU\", \"GRC\",\n",
    "    \"LUX\", \"ROM\", \"SVK\", \"SVN\", \"ISL\", \"CYP\",\n",
    "    \"MLT\", \"HRV\", \"LIE\", \"USA\"]\n",
    "\n",
    "cleaned_companies_eu__us_df = cleaned_companies_df[cleaned_companies_df['country_code'].isin(eu_us_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_companies_eu__us_df.info()\n",
    "print(cleaned_companies_eu__us_df.status.value_counts())\n",
    "print(cleaned_companies_eu__us_df.funding_rounds.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_companies_df.funding_rounds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting \"status\" column value counts after creating EU and US dataset\n",
    "cleaned_companies_eu__us_df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with removing \"operating\" companies in \"status\" column altogether\n",
    "#cleaned_companies_eu__us_df = cleaned_companies_eu__us_df[np.logical_not(\n",
    "#    cleaned_companies_eu__us_df.status == 'operating')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can either only take the non-operating companies to label\n",
    "# or all non-operating companies plus some operating depending\n",
    "# on a filter to see them as successful i.e. operating time,\n",
    "# money raised, number of funding rounds etc.\n",
    "\n",
    "# Create dataset containing only companies that failed or succeeded\n",
    "# i.e. companies that are closed, acquired or went through an IPO\n",
    "# As well as \"operating\" companies with more than 2 years of operations\n",
    "#status_companies_df = cleaned_companies_df[cleaned_companies_df['status'].isin([\"operating\"])]\n",
    "\n",
    "# Selecting companies with operating status with at least 3 funding rounds as they can be seen as \n",
    "# being successful and other opertating companies are excluded as it is too early to tell if they\n",
    "# are or are not successful\n",
    "status_companies_operating__eu_us_df = cleaned_companies_eu__us_df[(\n",
    "    cleaned_companies_eu__us_df[\"status\"] == \"operating\") & (cleaned_companies_eu__us_df[\"funding_rounds\"] >=3)]\n",
    "\n",
    "# Selecting all non-operating companies i.e. acquired, ipo, closed\n",
    "# as they can be seen as directly successful or unsuccessful\n",
    "status_companies_nonoperating_eu_us_df = cleaned_companies_eu__us_df[~cleaned_companies_eu__us_df['status'].isin([\"operating\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the operating with three rounds or more \n",
    "# with the non-operating companies\n",
    "status_companies__eu_us_df = pd.concat([status_companies_operating__eu_us_df, status_companies_nonoperating_eu_us_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(status_companies__eu_us_df.status.value_counts())\n",
    "print(status_companies__eu_us_df.funding_rounds.value_counts())\n",
    "print(status_companies__eu_us_df.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating binary label column where 0 = closed (unsuccessful) and \n",
    "# 1 = aqcquired/IPO/operating with 3 or more rounds of funding (successful) \n",
    "status_companies__eu_us_df[\"label\"] = 0\n",
    "\n",
    "status_companies__eu_us_df.loc[status_companies__eu_us_df.status == \"operating\", \"label\"] = 1\n",
    "status_companies__eu_us_df.loc[status_companies__eu_us_df.status == \"acquired\", \"label\"] = 1\n",
    "status_companies__eu_us_df.loc[status_companies__eu_us_df.status == \"ipo\", \"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and adding \"rounds\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rounds_df.shape)\n",
    "rounds_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_df.funding_round_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \"post_ipo_equity\" and \"post_ipo_debt\" rows from \"funding_round_type\" column\n",
    "cleaned_rounds_df = rounds_df[np.logical_and(\n",
    "    rounds_df.funding_round_type != 'post_ipo_equity',\n",
    "    rounds_df.funding_round_type != 'post_ipo_debt')]\n",
    "\n",
    "print(cleaned_rounds_df.funding_round_type.value_counts())\n",
    "print(cleaned_rounds_df.shape)\n",
    "\n",
    "print(\"\\n\", cleaned_rounds_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting \"funded_at\" column to Pandas Date Time Objects to perform future time operations\n",
    "cleaned_rounds_df.funded_at = pd.to_datetime(cleaned_rounds_df[\"funded_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rounds_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding \"time_between_rounds\" & \"raised_amount_usd\" columns as features to be used for the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting cleaned_rounds_df by company_name and funded_at\n",
    "cleaned_rounds_df.sort_values(by=[\"company_name\", \"funded_at\"], ascending=True, inplace=True)\n",
    "\n",
    "# Creating column for time between rounds\n",
    "cleaned_rounds_df[\"time_between_rounds\"] = cleaned_rounds_df.groupby(\"company_name\").funded_at.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average time between rounds and average round amount raised in USD\n",
    "avg_time_round = cleaned_rounds_df.groupby(\"company_name\").agg({\n",
    "    \"time_between_rounds\":pd.Series.mean, \"raised_amount_usd\": \"mean\"}).rename(\n",
    "    columns={\n",
    "        \"time_between_rounds\": \"avg_time_between_rounds\",\n",
    "        \"raised_amount_usd\": \"avg_raised_amount_usd\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into main status_companies_df dataframe\n",
    "status_companies__eu_us_df = status_companies__eu_us_df.merge(\n",
    "    avg_time_round, how='left', left_on='name', right_on='company_name')\n",
    "\n",
    "# Removing \"company_name\" column as it is redundant\n",
    "status_companies__eu_us_df = status_companies__eu_us_df.drop(\"company_name\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timedelta \"avg_time_between_rounds\" column to float values for classifiers later\n",
    "# This can be seen as number of days averaging between each funding round for the startup\n",
    "status_companies__eu_us_df[\"avg_time_between_rounds\"] = status_companies__eu_us_df.avg_time_between_rounds.dt.days\n",
    "\n",
    "status_companies__eu_us_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing \"category_list\" column to be more intuitive and give the single industry area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving on to \"category_list\" column which says the industry(ies) the startup is in\n",
    "\n",
    "status_companies__eu_us_df.category_list.isna().sum()\n",
    "\n",
    "# Filling empty categories with \"unknown\"\n",
    "status_companies__eu_us_df.category_list.fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column with list of categories\n",
    "status_companies__eu_us_df[\n",
    "    'cat_list'] = status_companies__eu_us_df.category_list.apply(str.split,\n",
    "                                                                sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Creating list of category lists\n",
    "cats = list(status_companies__eu_us_df.cat_list)\n",
    "\n",
    "# Flatten list\n",
    "flat_cats = [cat for sublist in cats for cat in sublist]\n",
    "\n",
    "# Counting occurences of each\n",
    "cat_counts = Counter(flat_cats).most_common()\n",
    "\n",
    "# Distribution\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar([x[0] for x in cat_counts[0:25]],\n",
    "        [x[1] for x in cat_counts[0:25]],\n",
    "        width=0.8)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('25 Top Industries', size=20)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking top 25 categories/industries\n",
    "top_cats = [x[0] for x in cat_counts[0:25]]\n",
    "\n",
    "# If a company has multiple categories listed, it replaces it with the ones it has in the top 25 only\n",
    "status_companies__eu_us_df.cat_list = status_companies__eu_us_df.cat_list.map(\n",
    "    lambda x: list(set(x) & set(top_cats))\n",
    "    if set(x) & set(top_cats) else ['0_other_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of category lists\n",
    "cats = list(status_companies__eu_us_df.cat_list)\n",
    "\n",
    "# Flatten list\n",
    "flat_cats = [cat for sublist in cats for cat in sublist]\n",
    "\n",
    "# Count occurences\n",
    "cat_counts = Counter(flat_cats).most_common()\n",
    "\n",
    "# Distribution\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar([x[0] for x in cat_counts[0:25]],\n",
    "        [x[1] for x in cat_counts[0:25]],\n",
    "        width=0.8)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top Industries', size=20)\n",
    "sns.set_theme()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For future use to create dummy variables in feature matrix\n",
    "status_companies__eu_us_df.cat_list = status_companies__eu_us_df.cat_list.apply(\n",
    "    lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Removing category_list\n",
    "\n",
    "status_companies__eu_us_df.drop([\"category_list\"], axis=1, inplace=True)\n",
    "\n",
    "# Renaming cat_list to Industry\n",
    "status_companies__eu_us_df.rename(columns={\"cat_list\": \"industry\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing blank space with underscore in \"industry\"\n",
    "status_companies__eu_us_df.industry.replace(\" \", \"_\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Investor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investments_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of investors per startup\n",
    "investor_count = investments_df[[\"company_permalink\",\n",
    "                         \"investor_permalink\"]].groupby([\"company_permalink\"]).agg([\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging DFs\n",
    "status_companies__eu_us_df = pd.merge(how=\"left\",left=status_companies__eu_us_df, right=investor_count, \n",
    "                                 left_on=\"permalink\", right_on=\"company_permalink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming \"investor_permalink\" to num_of_investors\n",
    "status_companies__eu_us_df.rename(columns={status_companies__eu_us_df.columns[-1]: \"num_of_investors\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.info()\n",
    "#status_companies_df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping permalink Column as they do not provide much\n",
    "status_companies__eu_us_df.drop(axis=1, columns=[\"permalink\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing NaN values to \"unknown\"  for \"country_code\"\n",
    "#status_companies__eu_us_df.country_code.fillna('Unknown', inplace=True)\n",
    "\n",
    "status_companies__eu_us_df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.info()\n",
    "\n",
    "\n",
    "# Countries Distribution\n",
    "\n",
    "country_dist = status_companies__eu_us_df.groupby(\n",
    "    'country_code').size().sort_values(ascending=False)\n",
    "plt.bar(country_dist[0:20].index, height=country_dist[0:20].values)\n",
    "plt.xticks(rotation=90)\n",
    "sns.despine()\n",
    "plt.title('Distribution of Top 10 Countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots to see skewness\n",
    "\n",
    "plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.boxplot(status_companies__eu_us_df.avg_time_between_rounds[status_companies__eu_us_df.avg_time_between_rounds.notnull()])\n",
    "plt.title(\"Average Time Between Rounds\", size=12)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.boxplot(status_companies__eu_us_df.avg_raised_amount_usd[status_companies__eu_us_df.avg_raised_amount_usd.notnull()])\n",
    "plt.title(\"Average Raised Amount USD\", size=10)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.boxplot(status_companies__eu_us_df.num_of_investors[status_companies__eu_us_df.num_of_investors.notnull()])\n",
    "plt.title(\"Number of Investors\", size=16)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.boxplot(status_companies__eu_us_df.funding_total_usd[status_companies__eu_us_df.funding_total_usd.notnull()])\n",
    "plt.title(\"Total Funding USD\", size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "status_companies__eu_us_df[\"avg_raised_amount_usd\"].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_companies__eu_us_df.iloc[9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to remove NaN\n",
    "#status_companies__eu_us_df.dropna(inplace=True)\n",
    "status_companies__eu_us_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locateOutliers(df, column, lim_scalar=1.5):\n",
    "    \"\"\"\n",
    "    Returns outliers above the max limit for a column in a dataframe\n",
    "\n",
    "    input: DataFrame, column(series),lim_scalar(float)\n",
    "    output: DataFrame\n",
    "    \"\"\"\n",
    "    q25, q50, q75 = df[column].quantile(q=[0.25, 0.5, 0.75])\n",
    "    iqr = q75 - q25\n",
    "    # max limits to be considered an outlier\n",
    "    max = q75 + lim_scalar * iqr\n",
    "    # identify the points\n",
    "    outlier_mask = [True if x > max else False for x in df[column]]\n",
    "    print(\n",
    "        \"{} outliers found out of {} data points, {:.2f}% of the data\".format(\n",
    "            sum(outlier_mask), len(df[column]),\n",
    "            100 * (sum(outlier_mask) / len(df[column]))))\n",
    "    return outlier_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Funding total USD: \")\n",
    "total_funding_outlier_mask = pd.Series(\n",
    "    locateOutliers(status_companies__eu_us_df, \"funding_total_usd\", lim_scalar=3))\n",
    "\n",
    "print(\"\\nAverage Raised Amount USD: \")\n",
    "avg_raise_outlier_mask = pd.Series(\n",
    "    locateOutliers(status_companies__eu_us_df, \"avg_raised_amount_usd\", lim_scalar=3))\n",
    "\n",
    "print(\"\\nTime between first rounds: \")\n",
    "first_time_outlier_mask = pd.Series(\n",
    "    locateOutliers(status_companies__eu_us_df, 'avg_time_between_rounds'))\n",
    "\n",
    "print(\"\\nNumber of Investors: \")\n",
    "num_of_investors_outlier_mask = pd.Series(\n",
    "    locateOutliers(status_companies__eu_us_df, \"num_of_investors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Removal\n",
    "clean_df1 = status_companies__eu_us_df[~total_funding_outlier_mask]\n",
    "clean_df2 = status_companies__eu_us_df[~avg_raise_outlier_mask]\n",
    "clean_df3 = status_companies__eu_us_df[~first_time_outlier_mask]\n",
    "clean_df4 = status_companies__eu_us_df[~num_of_investors_outlier_mask]\n",
    "\n",
    "clean_df = clean_df1.merge(clean_df2,\n",
    "                           left_on=list(status_companies__eu_us_df.columns),\n",
    "                           right_on=list(status_companies__eu_us_df.columns),\n",
    "                           how='inner')\n",
    "\n",
    "clean_df = clean_df.merge(clean_df3,\n",
    "                          left_on=list(status_companies__eu_us_df.columns),\n",
    "                          right_on=list(status_companies__eu_us_df.columns),\n",
    "                          how='inner')\n",
    "\n",
    "clean_df = clean_df.merge(clean_df4,\n",
    "                          left_on=list(status_companies__eu_us_df.columns),\n",
    "                          right_on=list(status_companies__eu_us_df.columns),\n",
    "                          how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removed {} outliers, {:.2f}% of the original dataset'.format(\n",
    "    (status_companies__eu_us_df.shape[0] - clean_df.shape[0]), 100 *\n",
    "    ((status_companies__eu_us_df.shape[0] - clean_df.shape[0]) / \n",
    "    (status_companies__eu_us_df.shape[0] + clean_df.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots to see skewness\n",
    "\n",
    "plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.boxplot(clean_df.avg_time_between_rounds[clean_df.avg_time_between_rounds.notnull()])\n",
    "plt.title(\"Average Time Between Rounds\", size=12)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.boxplot(clean_df.avg_raised_amount_usd[clean_df.avg_raised_amount_usd.notnull()])\n",
    "plt.title(\"Average Raised Amount USD\", size=10)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.boxplot(clean_df.num_of_investors[clean_df.num_of_investors.notnull()])\n",
    "plt.title(\"Number of Investors\", size=16)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.boxplot(clean_df.funding_total_usd[clean_df.funding_total_usd.notnull()])\n",
    "plt.title(\"Total Funding USD\", size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating between EU and US startups\n",
    "clean_eu = clean_df[~clean_df[\"country_code\"].isin([\"USA\"])]\n",
    "\n",
    "clean_us = clean_df[clean_df[\"country_code\"].isin([\"USA\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_eu.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_us.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_eu.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_us.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Value counts of EU labels:\\n \", clean_eu.label.value_counts())\n",
    "print(\"\\nValue counts of US labels:\\n\", clean_us.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filling \"avg_time_between_rounds\" NaN values with 0 as these \n",
    "# have only had 1 round of funding and therefor no time between rounds\n",
    "clean_eu[[\"avg_time_between_rounds\"]] = clean_eu[[\"avg_time_between_rounds\"]].fillna(value=0)\n",
    "clean_us[[\"avg_time_between_rounds\"]] = clean_us[[\"avg_time_between_rounds\"]].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_us.sample(10)\n",
    "\n",
    "# Dropping irrelevant columns for eu dataset:\n",
    "# \"name\", \"state_code\", \"region\"\n",
    "clean_eu = clean_eu.drop(columns=[\"name\", \"state_code\", \"region\", \"founded_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns for us dataset:\n",
    "# \"name\", \"country_code\", \"city\"\n",
    "clean_us = clean_us.drop(columns=[\"name\", \"country_code\", \"city\", \"founded_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaN \"region\" for US dataset as there are only 20 entries\n",
    "# and \"funding_total_usd\", \"avg_raised_amount_usd\" ,\"num_of_investors\"\n",
    "clean_us = clean_us.dropna(subset=[\"region\", \"funding_total_usd\", \"avg_raised_amount_usd\", \"num_of_investors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_eu.info()\n",
    "#clean_eu.isna().sum()\n",
    "#clean_eu[clean_eu['city'].isnull()]\n",
    "\n",
    "# Filling NaN \"cities\" with the most frequent city of the matching \"country_code\"\n",
    "#clean_eu[\"city\"] = clean_eu.set_index('country_code', append=True) \\\n",
    "#                    .groupby(level=\"country_code\").city.apply(lambda x: x.fillna(x.value_counts().idxmax())) \\\n",
    "#                    .reset_index('country_code', drop=True)\n",
    "\n",
    "# Dropping NaN \"city\" ,\"avg_raised_amount_usd\", \"num_of_investors\" for EU dataset\n",
    "clean_eu = clean_eu.dropna(subset=[\"city\", \"avg_raised_amount_usd\", \"num_of_investors\", \"funding_total_usd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_eu.isna().sum()\n",
    "#clean_eu.label.value_counts()\n",
    "#clean_us.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_eu.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_eu.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_us.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_eu.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving clean EU and US to csv in data folder\n",
    "clean_eu.to_csv('data/clean_eu.csv')\n",
    "clean_us.to_csv('data/clean_us.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

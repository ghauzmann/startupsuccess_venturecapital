{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score,\\\n",
    "precision_recall_curve, f1_score, fbeta_score,\\\n",
    "accuracy_score, confusion_matrix, roc_auc_score, roc_curve, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample, shuffle\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasets as csv\n",
    "\n",
    "eu_df = pd.read_csv(\"data/clean_eu.csv\")\n",
    "us_df = pd.read_csv(\"data/clean_us.csv\")\n",
    "\n",
    "# Dropping \"status\" column in both datasets as target boolean \"label\" took over \n",
    "# to indicate whether successful or not\n",
    "# Also dropping \"first_funding_at\" and \"last_funding_at\" as well as a random colum\n",
    "# \"Unnamed: 0\"\n",
    "\n",
    "eu_df = eu_df.drop([\"status\", \"first_funding_at\", \"last_funding_at\"], 1)\n",
    "eu_df = eu_df.drop(columns=eu_df.columns[0])\n",
    "\n",
    "us_df = us_df.drop([\"status\", \"first_funding_at\", \"last_funding_at\"], 1)\n",
    "us_df = us_df.drop(columns=us_df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dummies\n",
    "\n",
    "# EU\n",
    "# Industry Dummies\n",
    "industry_dummies_eu = pd.get_dummies(eu_df[\"industry\"], drop_first=True)\n",
    "# Country Dummies\n",
    "country_dummies_eu = pd.get_dummies(eu_df[\"country_code\"], drop_first=True)\n",
    "# City Dummies\n",
    "city_dummies_eu = pd.get_dummies(eu_df[\"city\"], drop_first=True)\n",
    "\n",
    "# US\n",
    "# Industry Dummies\n",
    "industry_dummies_us = pd.get_dummies(us_df[\"industry\"], drop_first=True)\n",
    "# State Dummies\n",
    "state_dummies_us = pd.get_dummies(us_df[\"state_code\"], drop_first=True)\n",
    "# City Dummies\n",
    "region_dummies_us = pd.get_dummies(us_df[\"region\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df[\"region\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting numerical columns\n",
    "\n",
    "X_col_num = [\n",
    "    'funding_rounds', \"funding_total_usd\", 'avg_time_between_rounds', 'avg_raised_amount_usd',\n",
    "    'num_of_investors']\n",
    "\n",
    "#EU\n",
    "X_num_eu = eu_df[X_col_num]\n",
    "\n",
    "#US\n",
    "X_num_us = us_df[X_col_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge feature matrix\n",
    "\n",
    "# EU\n",
    "X_eu = X_num_eu.merge(industry_dummies_eu, left_index=True, right_index=True).merge(\n",
    "    country_dummies_eu, left_index=True, right_index=True).merge(city_dummies_eu,\n",
    "                                                              left_index=True,\n",
    "                                                              right_index=True)\n",
    "\n",
    "# Add intercept column\n",
    "X_eu['intercept'] = 1\n",
    "\n",
    "\n",
    "# US\n",
    "X_us = X_num_us.merge(industry_dummies_us, left_index=True, right_index=True).merge(\n",
    "    state_dummies_us, left_index=True, right_index=True).merge(region_dummies_us,\n",
    "                                                              left_index=True,\n",
    "                                                              right_index=True)\n",
    "\n",
    "# Add intercept column\n",
    "X_us['intercept'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "\n",
    "# EU\n",
    "y_eu = eu_df.label\n",
    "\n",
    "# us\n",
    "y_us = us_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(eu_df[X_col_num + ['label']], hue='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a closer look at some of the features\n",
    "\n",
    "# Average raise amount\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.kdeplot(eu_df[eu_df.label == 0]['avg_raised_amount_usd'],\n",
    "            label='Fail',\n",
    "            shade=True,\n",
    "            color='teal')\n",
    "sns.kdeplot(eu_df[eu_df.label == 1]['avg_raised_amount_usd'],\n",
    "            label='Success',\n",
    "            shade=True,\n",
    "            color='royalblue')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Investors\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.kdeplot(eu_df[eu_df.label == 0]['num_of_investors'],\n",
    "            label='Fail',\n",
    "            shade=True,\n",
    "            color='teal')\n",
    "sns.kdeplot(eu_df[eu_df.label == 1]['num_of_investors'],\n",
    "            label='Success',\n",
    "            shade=True,\n",
    "            color='royalblue')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count target values\n",
    "\n",
    "#EU\n",
    "target_count_eu = y_eu.value_counts()\n",
    "\n",
    "# # print class balance\n",
    "print(\"EU: \")\n",
    "print(f'Class 0: {target_count_eu[0]}')\n",
    "print(f'Class 1: {target_count_eu[1]}')\n",
    "print(f'Proportion: {round(target_count_eu[0] / target_count_eu[1], 2)} : 1')\n",
    "print('Percentage of Majority Class: {:f}'.format(\n",
    "    round(target_count_eu[0] / sum(target_count_eu), 4) * 100))\n",
    "\n",
    "#US\n",
    "target_count_us = y_us.value_counts()\n",
    "\n",
    "# # print class balance\n",
    "print(\"\\nUS: \")\n",
    "print(f'Class 0: {target_count_us[0]}')\n",
    "print(f'Class 1: {target_count_us[1]}')\n",
    "print(f'Proportion: {round(target_count_us[0] / target_count_us[1], 2)} : 1')\n",
    "print('Percentage of Majority Class: {:f}'.format(\n",
    "    round(target_count_us[0] / sum(target_count_us), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "ada = ADASYN()\n",
    "\n",
    "# EU\n",
    "X_eu, y_eu = ada.fit_resample(X_eu, y_eu)\n",
    "\n",
    "# US\n",
    "X_us, y_us = ada.fit_resample(X_us, y_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count target values after Upsampling\n",
    "\n",
    "#EU\n",
    "target_count_upsampled_eu = y_eu.value_counts()\n",
    "\n",
    "# # print class balance\n",
    "print(\"EU: \")\n",
    "print(f'Class 0: {target_count_upsampled_eu[0]}')\n",
    "print(f'Class 1: {target_count_upsampled_eu[1]}')\n",
    "print(f'Proportion: {round(target_count_upsampled_eu[0] / target_count_upsampled_eu[1], 2)} : 1')\n",
    "print('Percentage of Majority Class: {:f}'.format(\n",
    "    round(target_count_upsampled_eu[0] / sum(target_count_upsampled_eu), 4) * 100))\n",
    "\n",
    "#US\n",
    "target_count_upsampled_us = y_us.value_counts()\n",
    "\n",
    "# # print class balance\n",
    "print(\"\\nUS: \")\n",
    "print(f'Class 0: {target_count_upsampled_us[0]}')\n",
    "print(f'Class 1: {target_count_upsampled_us[1]}')\n",
    "print(f'Proportion: {round(target_count_upsampled_us[0] / target_count_upsampled_us[1], 2)} : 1')\n",
    "print('Percentage of Majority Class: {:f}'.format(\n",
    "    round(target_count_upsampled_us[0] / sum(target_count_upsampled_us), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data with 80% to train and 20% to test\n",
    "# Stratify to ensure train and test sets have \n",
    "# similar proportions of either target class\n",
    "\n",
    "# EU\n",
    "X_train_eu, X_test_eu, y_train_eu, y_test_eu = train_test_split(X_eu,\n",
    "                                                    y_eu,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=40,\n",
    "                                                    stratify=y_eu)\n",
    "\n",
    "# US\n",
    "X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(X_us,\n",
    "                                                    y_us,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=40,\n",
    "                                                    stratify=y_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# EU\n",
    "\n",
    "# Fit the scaler using the training data and scale it\n",
    "X_train_scaled_eu = pd.DataFrame(scaler.fit_transform(X_train_eu.values),\n",
    "                              columns=X_eu.columns)\n",
    "\n",
    "# Scale the test data\n",
    "X_test_scaled_eu = pd.DataFrame(scaler.transform(X_test_eu.values),\n",
    "                             columns=X_eu.columns)\n",
    "\n",
    "# US\n",
    "\n",
    "# Fit the scaler using the training data and scale it\n",
    "X_train_scaled_us = pd.DataFrame(scaler.fit_transform(X_train_us.values),\n",
    "                              columns=X_us.columns)\n",
    "\n",
    "# Scale the test data\n",
    "X_test_scaled_us = pd.DataFrame(scaler.transform(X_test_us.values),\n",
    "                             columns=X_us.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# EU\n",
    "# Instantiate model\n",
    "logreg_eu = LogisticRegression(C=10, solver='lbfgs')\n",
    "\n",
    "# Fit model to the training data\n",
    "logreg_eu.fit(X_train_scaled_eu, y_train_eu)\n",
    "\n",
    "y_pred_lr_eu = logreg_eu.predict(X_test_scaled_eu)\n",
    "acc_lr_eu = accuracy_score(y_test_eu, y_pred_lr_eu)\n",
    "\n",
    "f1_lr_eu = f1_score(y_test_eu, y_pred_lr_eu)\n",
    "\n",
    "print(\"Logistic Regression EU Accuracy: \", acc_lr_eu)\n",
    "print(\"Logistic Regression EU f1 Score: \", f1_lr_eu)\n",
    "\n",
    "# Calculate ROC curve \n",
    "fpr_lr_eu, tpr_lr_eu, thresholds_lr_eu = roc_curve(\n",
    "    y_test_eu,\n",
    "    logreg_eu.predict_proba(X_test_scaled_eu)[:, 1])\n",
    "\n",
    "# Calculate area under the curve (AUC) for ROC\n",
    "auc_lr_eu = roc_auc_score(y_test_eu, logreg_eu.predict_proba(X_test_scaled_eu)[:, 1])\n",
    "\n",
    "\n",
    "# US\n",
    "# Instantiate model\n",
    "logreg_us = LogisticRegression(C=10, solver='lbfgs')\n",
    "\n",
    "# Fit model to the training data\n",
    "logreg_us.fit(X_train_scaled_us, y_train_us)\n",
    "\n",
    "y_pred_lr_us = logreg_us.predict(X_test_scaled_us)\n",
    "acc_lr_us = accuracy_score(y_test_us, y_pred_lr_us)\n",
    "\n",
    "f1_lr_us = f1_score(y_test_us, y_pred_lr_us)\n",
    "\n",
    "print(\"\\nLogistic Regression US Accuracy: \", acc_lr_us)\n",
    "print(\"Logistic Regression US f1 Score: \", f1_lr_us)\n",
    "\n",
    "# Calculate ROC curve \n",
    "fpr_lr_us, tpr_lr_us, thresholds_lr_us = roc_curve(\n",
    "    y_test_us,\n",
    "    logreg_us.predict_proba(X_test_scaled_us)[:, 1])\n",
    "\n",
    "# Calculate area under the curve (AUC) for ROC\n",
    "auc_lr_us = roc_auc_score(y_test_us, logreg_us.predict_proba(X_test_scaled_us)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "\n",
    "# EU\n",
    "lr_coefs_eu = list(zip(X_eu.columns, logreg_eu.coef_[0]))\n",
    "lr_coefs_df_eu = pd.DataFrame(lr_coefs_eu)\n",
    "lr_top_coefs_eu = [x for x in lr_coefs_eu if np.abs(x[1]) > .07]\n",
    "lr_top_coefs_eu = sorted(lr_top_coefs_eu, key=(lambda x: x[1]), reverse=True)\n",
    "lr_top_coefs_df_eu = pd.DataFrame(lr_top_coefs_eu)\n",
    "\n",
    "plt.barh([x[0] for x in lr_top_coefs_eu], width=[x[1] for x in lr_top_coefs_eu])\n",
    "plt.title('LogOdds')\n",
    "plt.grid(b=False)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "#EU\n",
    "svm_eu = svm.SVC(kernel=\"linear\", probability=True)\n",
    "svm_eu.fit(X_train_scaled_eu, y_train_eu)\n",
    "\n",
    "y_pred_svm_eu = svm_eu.predict(X_test_scaled_eu)\n",
    "acc_svm_eu = accuracy_score(y_test_eu, y_pred_svm_eu)\n",
    "\n",
    "f1_svm_eu = f1_score(y_test_eu, y_pred_svm_eu)\n",
    "\n",
    "print(\"SVM EU Accuracy: \", acc_svm_eu)\n",
    "print(\"SVM EU f1 Score: \", f1_svm_eu)\n",
    "\n",
    "# Calculate ROC curve \n",
    "fpr_svm_eu, tpr_svm_eu, thresholds_svm_eu = roc_curve(\n",
    "    y_test_eu,\n",
    "    svm_eu.predict_proba(X_test_scaled_eu)[:, 1])\n",
    "\n",
    "# Calculate area under the curve (AUC) for ROC\n",
    "auc_svm_eu = roc_auc_score(y_test_eu, svm_eu.predict_proba(X_test_scaled_eu)[:, 1])\n",
    "\n",
    "\n",
    "#US\n",
    "svm_us = svm.SVC(kernel=\"linear\", probability=True)\n",
    "svm_us.fit(X_train_scaled_us, y_train_us)\n",
    "\n",
    "y_pred_svm_us = svm_us.predict(X_test_scaled_us)\n",
    "acc_svm_us = accuracy_score(y_test_us, y_pred_svm_us)\n",
    "\n",
    "f1_svm_us = f1_score(y_test_us, y_pred_svm_us)\n",
    "\n",
    "print(\"SVM US Accuracy: \", acc_svm_us)\n",
    "print(\"SVM US f1 Score: \", f1_svm_us)\n",
    "\n",
    "# Calculate ROC curve \n",
    "fpr_svm_us, tpr_svm_us, thresholds_svm_us = roc_curve(\n",
    "    y_test_us,\n",
    "    svm_us.predict_proba(X_test_scaled_us)[:, 1])\n",
    "\n",
    "# Calculate area under the curve (AUC) for ROC\n",
    "auc_svm_us = roc_auc_score(y_test_us, svm_us.predict_proba(X_test_scaled_us)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# EU\n",
    "rf_eu = RandomForestClassifier(n_estimators=500,\n",
    "                            bootstrap=True,\n",
    "                            oob_score=True,\n",
    "                            random_state=1234,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "# fit\n",
    "rf_eu.fit(X_train_scaled_eu, y_train_eu)\n",
    "\n",
    "y_pred_rf_eu = rf_eu.predict(X_test_scaled_eu)\n",
    "acc_rf_eu = accuracy_score(y_test_eu, y_pred_rf_eu)\n",
    "\n",
    "f1_rf_eu = f1_score(y_test_eu, y_pred_rf_eu)\n",
    "\n",
    "print(\"Random Forest EU Accuracy: \", acc_rf_eu)\n",
    "print(\"Random Forest EU f1 Score: \", f1_rf_eu)\n",
    "\n",
    "# Calculate ROC curve \n",
    "fpr_rf_eu, tpr_rf_eu, thresholds_rf_eu = roc_curve(\n",
    "    y_test_eu,\n",
    "    rf_eu.predict_proba(X_test_scaled_eu)[:, 1])\n",
    "\n",
    "# Calculate area under the curve (AUC) for ROC\n",
    "auc_rf_eu = roc_auc_score(y_test_eu, rf_eu.predict_proba(X_test_scaled_eu)[:, 1])\n",
    "\n",
    "\n",
    "# US\n",
    "rf_us = RandomForestClassifier(n_estimators=500,\n",
    "                            bootstrap=True,\n",
    "                            oob_score=True,\n",
    "                            random_state=1234,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "# fit\n",
    "rf_us.fit(X_train_scaled_us, y_train_us)\n",
    "\n",
    "y_pred_rf_us = rf_us.predict(X_test_scaled_us)\n",
    "acc_rf_us = accuracy_score(y_test_us, y_pred_rf_us)\n",
    "\n",
    "f1_rf_us = f1_score(y_test_us, y_pred_rf_us)\n",
    "\n",
    "print(\"Random Forest US Accuracy: \", acc_rf_us)\n",
    "print(\"Random Forest US f1 Score: \", f1_rf_us)\n",
    "\n",
    "# Calculate ROC curve \n",
    "fpr_rf_us, tpr_rf_us, thresholds_rf_us = roc_curve(\n",
    "    y_test_us,\n",
    "    rf_us.predict_proba(X_test_scaled_us)[:, 1])\n",
    "\n",
    "# Calculate area under the curve (AUC) for ROC\n",
    "auc_rf_us = roc_auc_score(y_test_us, rf_us.predict_proba(X_test_scaled_us)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "xgb_eu = XGBClassifier()\n",
    "xgb_eu.fit(X_train_scaled_eu, y_train_eu)\n",
    "\n",
    "y_pred_xgb_eu = xgb_eu.predict(X_test_scaled_eu)\n",
    "acc_xgb_eu = accuracy_score(y_test_eu, y_pred_xgb_eu)\n",
    "\n",
    "f1_xgb_eu = f1_score(y_test_eu, y_pred_xgb_eu)\n",
    "\n",
    "print(\"XGBoost EU Accuracy: \", acc_xgb_eu)\n",
    "print(\"XGBoost EU f1 Score: \", f1_xgb_eu)\n",
    "\n",
    "# Calculate ROC curve \n",
    "fpr_xgb_eu, tpr_xgb_eu, thresholds_xgb_eu = roc_curve(\n",
    "    y_test_eu,\n",
    "    xgb_eu.predict_proba(X_test_scaled_eu)[:, 1])\n",
    "\n",
    "# Calculate area under the curve (AUC) for ROC\n",
    "auc_xgb_eu = roc_auc_score(y_test_eu, xgb_eu.predict_proba(X_test_scaled_eu)[:, 1])\n",
    "\n",
    "\n",
    "# US\n",
    "xgb_us = RandomForestClassifier(n_estimators=500,\n",
    "                            bootstrap=True,\n",
    "                            oob_score=True,\n",
    "                            random_state=1234,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "# fit\n",
    "xgb_us.fit(X_train_scaled_us, y_train_us)\n",
    "\n",
    "y_pred_xgb_us = xgb_us.predict(X_test_scaled_us)\n",
    "acc_xgb_us = accuracy_score(y_test_us, y_pred_xgb_us)\n",
    "\n",
    "f1_xgb_us = f1_score(y_test_us, y_pred_xgb_us)\n",
    "\n",
    "print(\"XGBoost US Accuracy: \", acc_xgb_us)\n",
    "print(\"XGBoost US f1 Score: \", f1_xgb_us)\n",
    "\n",
    "# Calculate ROC curve \n",
    "fpr_xgb_us, tpr_xgb_us, thresholds_xgb_us = roc_curve(\n",
    "    y_test_us,\n",
    "    xgb_us.predict_proba(X_test_scaled_us)[:, 1])\n",
    "\n",
    "# Calculate area under the curve (AUC) for ROC\n",
    "auc_xgb_us = roc_auc_score(y_test_us, xgb_us.predict_proba(X_test_scaled_us)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance extraction\n",
    "\n",
    "# Random Forest\n",
    "# EU\n",
    "rf_eu_feats = pd.DataFrame({\n",
    "    'feature': X_eu.columns,\n",
    "    'importance': rf_eu.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Look at top 10 features\n",
    "print(\"Feature Importance Random Forest top 10 EU: \")\n",
    "rf_eu_feats[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# US\n",
    "rf_us_feats = pd.DataFrame({\n",
    "    'feature': X_us.columns,\n",
    "    'importance': rf_us.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Look at top 10 features\n",
    "print(\"Feature Importance Random Forest top 10 US: \")\n",
    "rf_us_feats[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# eu\n",
    "xgb_eu_feats = pd.DataFrame({\n",
    "    'feature': X_eu.columns,\n",
    "    'importance': xgb_eu.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Look at top 10 features\n",
    "print(\"Feature Importance XGBoost top 10 EU: \")\n",
    "xgb_eu_feats[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# US\n",
    "xgb_us_feats = pd.DataFrame({\n",
    "    'feature': X_us.columns,\n",
    "    'importance': xgb_us.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Look at top 10 features\n",
    "print(\"Feature Importance XGBoost top 10 US: \")\n",
    "xgb_us_feats[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison EU\n",
    "models = ['lr', 'svm', 'rf', 'xgb']\n",
    "\n",
    "model_acc_eu = [\n",
    "    acc_lr_eu, acc_svm_eu, acc_rf_eu, acc_xgb_eu]\n",
    "\n",
    "model_aucs_eu = [\n",
    "    auc_lr_eu, auc_svm_eu, auc_rf_eu, auc_xgb_eu]\n",
    "\n",
    "model_fbetas_eu = [f1_lr_eu, f1_svm_eu, f1_rf_eu, f1_xgb_eu]\n",
    "\n",
    "model_names = [\n",
    "    'Logistic Regression', 'Support Vector Machine', 'Random Forest', 'XGBoost',]\n",
    "\n",
    "# Plot ROC Curves\n",
    "\n",
    "plt.plot(fpr_lr_eu, tpr_lr_eu, lw=1, label='Logistic Regression')\n",
    "plt.plot(fpr_svm_eu, tpr_svm_eu, lw=1, label='SVM')\n",
    "plt.plot(fpr_rf_eu, tpr_rf_eu, lw=1, label='Random Forest')\n",
    "plt.plot(fpr_xgb_eu, tpr_xgb_eu, lw=1, label='XGBoost')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], c='violet', ls='--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Model Comparison EU - ROC curve')\n",
    "plt.legend(ncol=2, fontsize='small')\n",
    "sns.despine()\n",
    "\n",
    "# Print EU AUC Scores\n",
    "for model in list(zip(model_names, model_aucs_eu)):\n",
    "    print(\"EU ROC AUC score = {:3f} for {}\".format(model[1], model[0]))\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Print EU Accuracy Scores\n",
    "for model in list(zip(model_names, model_acc_eu)):\n",
    "    print(\"EU Accuracy score = {:3f} for {}\".format(model[1], model[0]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print EU f1 Scores\n",
    "for model in list(zip(model_names, model_fbetas_eu)):\n",
    "    print(\"EU f1 score = {:3f} for {}\".format(model[1], model[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison US\n",
    "model_acc_us = [\n",
    "    acc_lr_us, acc_svm_us, acc_rf_us, acc_xgb_us]\n",
    "\n",
    "model_aucs_us = [\n",
    "    auc_lr_us, auc_svm_us, auc_rf_us, auc_xgb_us]\n",
    "\n",
    "model_fbetas_us = [f1_lr_us, f1_svm_us, f1_rf_us, f1_xgb_us]\n",
    "\n",
    "# Plot ROC Curves\n",
    "\n",
    "plt.plot(fpr_lr_us, tpr_lr_us, lw=1, label='Logistic Regression')\n",
    "plt.plot(fpr_svm_us, tpr_svm_us, lw=1, label='SVM')\n",
    "plt.plot(fpr_rf_us, tpr_rf_us, lw=1, label='Random Forest')\n",
    "plt.plot(fpr_xgb_us, tpr_xgb_us, lw=1, label='XGBoost')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], c='violet', ls='--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Model Comparison US - ROC curve')\n",
    "plt.legend(ncol=2, fontsize='small')\n",
    "sns.despine()\n",
    "\n",
    "# Print EU AUC Scores\n",
    "for model in list(zip(model_names, model_aucs_us)):\n",
    "    print(\"US ROC AUC score = {:3f} for {}\".format(model[1], model[0]))\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Print EU Accuracy Scores\n",
    "for model in list(zip(model_names, model_acc_us)):\n",
    "    print(\"US Accuracy score = {:3f} for {}\".format(model[1], model[0]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print EU f1 Scores\n",
    "for model in list(zip(model_names, model_fbetas_us)):\n",
    "    print(\"US f1 score = {:3f} for {}\".format(model[1], model[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
